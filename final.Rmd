---
title: "PH.140.644_HW3"
author: "Kunyu An"
output:
  pdf_document:
    toc_depth: 2
    latex_engine : xelatex
  html_document:
    toc_depth: '2'
    df_print: paged
header-includes:
- \usepackage{amsmath,latexsym,amsfonts,amsthm,cleveref}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


```{r load_libraries,include=FALSE}
library(tidyverse)
library(ISLR)
library(MASS) 
library(class)
library(randomForest)
```

In this project, my goal is to analyze `NHANES 2003-2004` data and build a model to predict the mortality status for participants 50 years and older. I plan to divide this project into three parts. The first is the data pre-processing part, in which I will clean the data, determine the predictors as well as split the training and testing sets. The second part is to build a classification model to do the prediction. Finally, I will explore and preform some statistical analysis on the results.

### Data Preprocessing

We first load the data set. There are 10122 observations and 813 variables.
```{r}
load(file = "nhanes2003-2004.Rda")
```

Now we are going to make some modification on our data.

- First I dropped out the data with no mortality status.

- The variable `RIDAGEEX` tells use the age in months at the time of examination. Since our goal is to predict the mortality status for participants over 50 years old, we are looking for the data with `RIDAGEEX` greater than $50 * 12 =$ 600 months. 

```{r}
data <- nhanes2003_2004[is.na(nhanes2003_2004$mortstat)!=TRUE,]
data <- data %>% filter(as.numeric(RIDAGEEX) >= 600)
```

We certainly do not want to use all 813 variables, below are the most informative variables that will be used as predictors in this project.

Note that there're some missing data that should be dropped out when training our models.

```{r}
labels <- c("mortstat", "RIDAGEYR", "RIAGENDR", "BPQ010", "BPQ060", "DIQ010", "DIQ050", "DIQ090", "MCQ010", "MCQ053", "MCQ160A", "MCQ160B", "MCQ160K", "MCQ160L", "BMXWAIST", "MCQ160M", "MCQ220", "MCQ245A", "MCQ250A", "MCQ250B", "MCQ250C", "MCQ250E", "MCQ250F", "MCQ250G", "MCQ265", "SSQ011", "SSQ051", "WHQ030", "WHQ040", "LBXRDW", "HSD010", "BPXPULS", "BPXML1", "VIQ200", "BMXBMI", "BPXSY1", "BPXDI1")
data <- data[,labels]
data <- na.omit(data)
```

Now we can split the data into training and testing set. I decided to take 70% as training set and 30% as testing set.

```{r}
set.seed(1)
train <- sample(nrow(data),0.70*nrow(data))
data.train <- data[train,]
data.test <- data[-train,]
```

### Fitting the model

For this part, I'm using three kinds of classification models: SVM and Logistic Regression. sensitivity and specificity will be calculated to decide which model works the best.

#### SVM:

- Use the `tune()` function to select an optimal cost. Consider values in the range 0.01 to 10.

```{r}
library("e1071")
tune.out <- tune(svm, mortstat~ ., data = data.train, kernel = "linear", ranges = list(cost = 10^seq(-2, 1, by = 0.25)))
summary(tune.out)
```

- Compute the sensitivity and specificity using the best cost.

```{r}
svm.linear = svm(mortstat ~ ., kernel = "linear", data = data.train, cost =tune.out$best.parameters$cost)
test.pred = predict(svm.linear, data.test)
test.pred <- ifelse(test.pred > 0.5, yes = 1,  no = 0)
table(data.test$mortstat, test.pred)
```

According to the table above, we can compute the sensitivity & specificity.
- Sensitivity : TP / TP + FN = $338 / (338 + 61) =$ `r 338 / (338 + 61)`
- Specificity : TN / FP + TN = $7/ (7 + 6) =$ `r 7/ (7 + 6)`

#### Logistic Regression: 
