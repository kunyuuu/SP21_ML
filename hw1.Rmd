---
title: "HW1"
author: "Kunyu An"
date: "2/12/2021"
output:
  pdf_document:
    df_print: paged
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


```{r load_libraries}
library(ISLR)
```
###Chapter 3 
####Q5

we want to show that $\hat{y}_{i} = \sum_{i'=1}^n a_{i'}y_{i'}$. Given that $\hat{\beta} = (\sum_{i=1}^n x_{i}y_{i})/(\sum_{i'=1}^n x_{i'}^2) $, we have


<center>$\hat{y}_{i} = x_{i}\hat{\beta} = x_{i}\frac{\sum_{i=1}^n x_{i}y_{i}}{\sum_{i'=1}^n x_{i'}^2} = \frac{x_{i}}{\sum_{i'=1}^n x_{i'}^2}\sum_{i=1}^n x_{i}y_{i}$</center>

Since $\frac{x_{i}}{\sum_{i'=1}^n x_{i'}^2}$ is a constant for any i', we can represent it as a constant $C_{i'}$. Then we get,

<center>$\hat{y}_{i} = C_{i'}\sum_{i=1}^n x_{i}y_{i} = \sum_{i=1}^n C_{i'}x_{i}y_{i}$</center>

Let $a_{i'} = C_{i'}x_{i}$, we get,

<center>$\hat{y}_{i} = \sum_{i=1}^n C_{i'}x_{i}y_{i} =  \sum_{i'=1}^n a_{i'}y_{i'}$</center>

####Q6

For simple linear regression, $\hat{y} = \hat{\beta_{0}}+ \hat{\beta_{1}} x$. According to (3.4), $\hat{\beta_{0}} = \bar{y} - \hat{\beta_{1}} \bar{x}$.Thus, we have,

<center>$\hat{y} = \bar{y} - \hat{\beta_{1}} \bar{x} + \hat{\beta_{1}} x =  \bar{y} - \hat{\beta_{1}}(x-\bar{x})$</center>

Let $x = \bar{x}$, we get $\hat{y} = \bar{y}$. Thus, we can say that  the least squares line always passes through the point $(\bar{x},\bar{y})$.

#### Q9
(a) Produce a scatterplot matrix which includes all of the variables in the data set.
```{r}
pairs(Auto)
```
(b) Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.
```{r}
names(Auto)
```
```{r}
new_data = Auto[1:8]
cor(new_data)
```
(c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance:
```{r}
fit_mpg <- lm(mpg~., data=new_data)
summary(fit_mpg)
```
##### i. Is there a relationship between the predictors and the response?
Yes. Because F-static is large and p-value is small, we can say there is there a relationship between the predictors and the response.
##### ii. Which predictors appear to have a statistically significant relationship to the response?
Year, weight, origin and displacement appear to have a statistically significant relationship to the response.
##### iii. What does the coefficient for the year variable suggest?
The positive coefficient for year indicates year and mpg have the same trend. In other words, we can get larger value of mpg with larger value of year.

#### (d) Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?
```{r}
par(mfrow=c(2,2))
plot(fit_mpg)
```
Point 320, point 323 and point 327 have unusually large residual.
Point 14 has unusually high leverage. 

##### (e) Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?


##### (f) Try a few different transformations of the variables, such as log(X), âˆšX, X2. Comment on your findings.


###Chapter 4
####Q1
We can start with the logistic function representation,

<center>$p(X) = \frac{e^{\beta_{0}+\beta_{1}^X}}{1+e^{\beta_{0}+\beta_{1}^X}}$</center>

Multiplying both side by ${1+e^{\beta_{0}+\beta_{1}^X}$, we get

<center>$e^{\beta_{0}+\beta_{1}^X} = p(X)(1+e^{\beta_{0}+\beta_{1}^X)} = p(X) + p(X)e^{\beta_{0}+\beta_{1}^X}$</center>

By subtract $p(X)e^{\beta_{0}+\beta_{1}^X}$ on both side, we get

<center>$e^{\beta_{0}+\beta_{1}^X}-p(X)e^{\beta_{0}+\beta_{1}^X} = e^{\beta_{0}+\beta_{1}^X}(1-p(X)) = p(X)$</center>

which can be simplified as,

<center>$\frac{p(X)}{1-p(X)} = e^{\beta_{0}+\beta_{1}^X}$ (the logit representation)</center>

####Q8
For KNN test, if K = 1, the training error should always be 0 and since the average error is 18%, the actually test error should be 36%, which is greatly than the 30% test error of logistic regression. Thus, we should use logistic regression for this dataset rather than KNN with K =1. 