<<<<<<< HEAD
---
title: "PH.140.644_HW1"
author: "Kunyu An"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


```{r load_libraries}
library(ISLR)
```
###Chapter 3 
####Q5

we want to show that $\hat{y}_{i} = \sum_{i'=1}^n a_{i'}y_{i'}$. Given that $\hat{\beta} = (\sum_{i=1}^n x_{i}y_{i})/(\sum_{i'=1}^n x_{i'}^2) $, we have


<center>$\hat{y}_{i} = x_{i}\hat{\beta} = x_{i}\frac{\sum_{i=1}^n x_{i}y_{i}}{\sum_{i'=1}^n x_{i'}^2} = \frac{x_{i}}{\sum_{i'=1}^n x_{i'}^2}\sum_{i=1}^n x_{i}y_{i}$</center>

Since $\frac{x_{i}}{\sum_{i'=1}^n x_{i'}^2}$ is a constant for any i', we can represent it as a constant $C_{i'}$. Then we get,

<center>$\hat{y}_{i} = C_{i'}\sum_{i=1}^n x_{i}y_{i} = \sum_{i=1}^n C_{i'}x_{i}y_{i}$</center>

Let $a_{i'} = C_{i'}x_{i}$, we get,

<center>$\hat{y}_{i} = \sum_{i=1}^n C_{i'}x_{i}y_{i} =  \sum_{i'=1}^n a_{i'}y_{i'}$</center>

####Q6

For simple linear regression, $\hat{y} = \hat{\beta_{0}}+ \hat{\beta_{1}} x$. According to (3.4), $\hat{\beta_{0}} = \bar{y} - \hat{\beta_{1}} \bar{x}$.Thus, we have,

<center>$\hat{y} = \bar{y} - \hat{\beta_{1}} \bar{x} + \hat{\beta_{1}} x =  \bar{y} - \hat{\beta_{1}}(x-\bar{x})$</center>

Let $x = \bar{x}$, we get $\hat{y} = \bar{y}$. Thus, we can say that  the least squares line always passes through the point $(\bar{x},\bar{y})$.

####Q9
(a) Produce a scatterplot matrix which includes all of the variables in the data set.
```{r}
pairs(Auto)
```
(b) Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.
```{r}
names(Auto)
```
```{r}
new_data = Auto[1:8]
cor(new_data)
```
(c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance:
```{r}
fit_mpg <- lm(mpg~., data=new_data)
summary(fit_mpg)
```
##### i. Is there a relationship between the predictors and the response?
Yes. Because F-static is large and p-value is small, we can say there is there a relationship between the predictors and the response.
##### ii. Which predictors appear to have a statistically significant relationship to the response?
Year, weight, origin and displacement appear to have a statistically significant relationship to the response.
##### iii. What does the coefficient for the year variable suggest?
The positive coefficient for year indicates year and mpg have the same trend. In other words, we can get larger value of mpg with larger value of year.

#### (d) Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?
```{r}
par(mfrow=c(2,2))
plot(fit_mpg)
```
Point 320, point 323 and point 327 have unusually large residual.
Point 14 has unusually high leverage. 

##### (e) Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?


##### (f) Try a few different transformations of the variables, such as log(X), √X, X2. Comment on your findings.


####Q15
This problem involves the Boston data set. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.

a. For each predictor, fit a simple linear regression model to predict the response.

We first launch our dataset and take a first look of our dataset.
```{r}
library(MASS)
attach(Boston)
head(Boston)
```

```{r}
fit_zn = lm(crim~zn, data = Boston)
summary(fit_zn)
```
For the predictor "zn", the p value is less than 0.05, that means we have strong evidence that there's statistically significant association between "zn" and "crim".

```{r}
fit_indus = lm(crim~indus, data = Boston)
summary(fit_indus)
```
For the predictor "indus", the p value is less than 0.05, that means we have strong evidence that there's statistically significant association between "indus" and "crim".

```{r}
fit_chas = lm(crim~chas, data = Boston)
summary(fit_chas)
```
For the predictor "chas", the p value 0.209 is greater than 0.05, that means we *do not have* enough evidence to conclude there's statistically significant association between "chas" and "crim".

```{r}
fit_nox = lm(crim~nox, data = Boston)
summary(fit_nox)
```
For the predictor "nox", the p value is less than 0.05, that means we have strong evidence that there's statistically significant association between "nox" and "crim".

```{r}
fit_rm = lm(crim~rm, data = Boston)
summary(fit_rm)
```
For the predictor "rm", the p value is less than 0.05, that means we have strong evidence that there's statistically significant association between "rm" and "crim".

```{r}
fit_age = lm(crim~age, data = Boston)
summary(fit_age)
```
For the predictor "age", the p value is less than 0.05, that means we have strong evidence that there's statistically significant association between "age" and "crim".

```{r}
fit_dis = lm(crim~dis, data = Boston)
summary(fit_dis)
```
For the predictor "dis", the p value is less than 0.05, that means we have strong evidence that there's statistically significant association between "dis" and "crim".

```{r}
fit_rad = lm(crim~rad, data = Boston)
summary(fit_rad)
```
For the predictor "rad", the p value is less than 0.05, that means we have strong evidence that there's statistically significant association between "rad" and "crim".

```{r}
fit_tax = lm(crim~tax, data = Boston)
summary(fit_tax)
```
For the predictor "tax", the p value is less than 0.05, that means we have strong evidence that there's statistically significant association between "tax" and "crim".

```{r}
fit_ptratio = lm(crim~ptratio, data = Boston)
summary(fit_ptratio)
```
For the predictor "ptratio", the p value is less than 0.05, that means we have strong evidence that there's statistically significant association between "ptratio" and "crim".

```{r}
fit_black = lm(crim~black, data = Boston)
summary(fit_black)
```
For the predictor "black", the p value is less than 0.05, that means we have strong evidence that there's statistically significant association between "black" and "crim".

```{r}
fit_lstat = lm(crim~lstat, data = Boston)
summary(fit_lstat)
```
For the predictor "lstat", the p value is less than 0.05, that means we have strong evidence that there's statistically significant association between "lstat" and "crim".

```{r}
fit_medv = lm(crim~medv, data = Boston)
summary(fit_medv)
```
For the predictor "medv", the p value is less than 0.05, that means we have strong evidence that there's statistically significant association between "medv" and "crim".

**To sum up, all the variables, except the "chas", have statistically significant association with the respond "crim".**

b. Fit a multiple regression model to predict the response using all of the predictors. Describe your results.

```{r}
fit_multi = lm(crim ~ ., data = Boston)
summary(fit_multi)
```
**According to the p values shown above, only "zn", "dis", "rad", "black" and "medv" have strong evidence to reject the null hypothesis since their p values are less than 0.05.**

c. How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regres- sion model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.

```{r}
x = c(coefficients(fit_zn)[2],
      coefficients(fit_indus)[2],
      coefficients(fit_chas)[2],
      coefficients(fit_nox)[2],
      coefficients(fit_rm)[2],
      coefficients(fit_age)[2],
      coefficients(fit_dis)[2],
      coefficients(fit_rad)[2],
      coefficients(fit_tax)[2],
      coefficients(fit_ptratio)[2],
      coefficients(fit_black)[2],
      coefficients(fit_lstat)[2],
      coefficients(fit_medv)[2])
y = coefficients(fit_multi)[2:14]
plot(x, y)
```
**From the figure above, we can see that there are differences between simple and multiple regression coefficients. The simple linear regression only being affected by a single variable but the multiple regression is affected by the average effect of multiple variables**

d.Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form $Y = \beta_{0} + \beta_{1}X + \beta_{2}X^2 + \beta_{3}X^3 + \epsilon $.

```{r}
fit_zn = lm(crim~poly(zn,3), data = Boston)
summary(fit_zn)
```
For the predictor "zn", the p value of quadratic coefficient is less than 0.05 but p values the cubic coefficient is greater than 0.05. That means there's a nonlinear association between "zn" and "crim" but the evidence is not strong.

```{r}
fit_indus = lm(crim~poly(indus,3), data = Boston)
summary(fit_indus)
```
For the predictor "indus", the p value of both quadratic and cubic coefficient is less than 0.05. That means there's strong evidence to show a nonlinear association between "indus" and "crim".

```{r}
fit_nox = lm(crim~poly(nox,3), data = Boston)
summary(fit_nox)
```
For the predictor "nox", the p value of quadratic and cubic coefficient is less than 0.05. That means there's strong evidence to show a nonlinear association between "nox" and "crim".

```{r}
fit_rm = lm(crim~poly(rm,3), data = Boston)
summary(fit_rm)
```
For the predictor "rm", the p value of quadratic coefficient is less than 0.05 but p values the cubic coefficient is greater than 0.05. That means there's a nonlinear association between "rm" and "crim" but the evidence is not strong.

```{r}
fit_age = lm(crim~poly(age,3), data = Boston)
summary(fit_age)
```
For the predictor "age", the p value of both quadratic and cubic coefficient is less than 0.05. That means there's strong evidence to show a nonlinear association between "age" and "crim".

```{r}
fit_dis = lm(crim~poly(dis,3), data = Boston)
summary(fit_dis)
```
For the predictor "dis", the p value of both quadratic and cubic coefficient is less than 0.05. That means there's strong evidence to show a nonlinear association between "dis" and "crim".

```{r}
fit_rad = lm(crim~poly(rad,3), data = Boston)
summary(fit_rad)
```
For the predictor "rad", the p value of quadratic coefficient is less than 0.05 but p values the cubic coefficient is greater than 0.05. That means there's a nonlinear association between "rad" and "crim" but the evidence is not strong.

```{r}
fit_tax = lm(crim~poly(tax,3), data = Boston)
summary(fit_tax)
```
For the predictor "tax", the p value of quadratic coefficient is less than 0.05 but p values the cubic coefficient is greater than 0.05. That means there's a nonlinear association between "tax" and "crim" but the evidence is not strong.

```{r}
fit_ptratio = lm(crim~poly(ptratio,3), data = Boston)
summary(fit_ptratio)
```
For the predictor "ptratio", the p value of both quadratic and cubic coefficient is less than 0.05. That means there's strong evidence to show a nonlinear association between "ptratio" and "crim".

```{r}
fit_black = lm(crim~poly(black,3), data = Boston)
summary(fit_black)
```
For the predictor "black", the p value of both quadratic and cubic coefficient is greater than 0.05. That means there's no enough evidence to conclude a nonlinear association between "black" and "crim".

```{r}
fit_lstat = lm(crim~poly(lstat,3), data = Boston)
summary(fit_lstat)
```
For the predictor "lastat", the p value of quadratic coefficient is less than 0.05 but p values the cubic coefficient is greater than 0.05. That means there's a nonlinear association between "lstat" and "crim" but the evidence is not strong.

```{r}
fit_medv = lm(crim~poly(medv,3), data = Boston)
summary(fit_medv)
```
For the predictor "medv", the p value of both quadratic and cubic coefficient is less than 0.05. That means there's strong evidence to show a nonlinear association between "medv" and "crim".

**To sum up, for variables “zn”, “rm”, “rad”, “tax” and “lstat”, their p-values suggest that the cubic coefficient is not statistically significant.For “indus”, “nox”, “age”, “dis”, “ptratio” and “medv”, their p-values suggest there's strong evidence to show a nonlinear association; for the variable “black”, the p-values suggest that both quandratic and cubic coefficients are not statistically significant and thus there is no nonlinear association.**

###Chapter 4
####Q1
We can start with the logistic function representation,

<center>$p(X) = \frac{e^{\beta_{0}+\beta_{1}^X}}{1+e^{\beta_{0}+\beta_{1}^X}}$</center>

Multiplying both side by ${1+e^{\beta_{0}+\beta_{1}^X}$, we get

<center>$e^{\beta_{0}+\beta_{1}^X} = p(X)(1+e^{\beta_{0}+\beta_{1}^X)} = p(X) + p(X)e^{\beta_{0}+\beta_{1}^X}$</center>

By subtract $p(X)e^{\beta_{0}+\beta_{1}^X}$ on both side, we get

<center>$e^{\beta_{0}+\beta_{1}^X}-p(X)e^{\beta_{0}+\beta_{1}^X} = e^{\beta_{0}+\beta_{1}^X}(1-p(X)) = p(X)$</center>

which can be simplified as,

<center>$\frac{p(X)}{1-p(X)} = e^{\beta_{0}+\beta_{1}^X}$ (the logit representation)</center>

####Q8
For KNN test, if K = 1, the training error should always be 0 and since the average error is 18%, the actually test error should be 36%, which is greatly than the 30% test error of logistic regression. Thus, we should use logistic regression for this dataset rather than KNN with K =1. 

####Q10

####Q11
a. Create a binary variable, `mpg01`, that contains a 1 if `mpg` contains a value above its median, and a 0 if `mpg` contains a value below its median. You can compute the median using the `median()` function. Note you may find it helpful to use the `data.frame()` function to create a single data set containing both `mpg01` and the other `Auto` variables.
```{r}
library(ISLR)
attach(Auto)
mpg01 = rep(0, length(mpg))
mpg01[mpg > median(mpg)] = 1
Auto = data.frame(Auto, mpg01)
head(Auto)
```

b.Explore the data graphically in order to investigate the association between `mpg01` and the other features. Which of the other features seem most likely to be useful in predicting `mpg01`? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.

We first can plot the boxplots for each variable.
```{r}
par(mfrow = c(2, 3))
plot(factor(mpg01), cylinders, xlab = "mpg01", ylab = "engine cylinders")
plot(factor(mpg01), displacement, xlab = "mpg01",ylab = "Engine displacement")
plot(factor(mpg01), horsepower,xlab = "mpg01",ylab = "Horsepower")
plot(factor(mpg01), weight, xlab = "mpg01",ylab = "Weight")
plot(factor(mpg01), acceleration, xlab = "mpg01",ylab = "acceleration")
plot(factor(mpg01), year, xlab = "mpg01", ylab = "Manufacture year")
mtext("Boxplots", outer = TRUE, line = -1.5)
```
According the boxplots above, we can see that the predictor `cylinders`,`displacement`,`horsepower` and `weight`are strongly associate with `mpg01`.

Then we can plot the scartterplox for more information.
```{r}
par(mfrow = c(2, 3))
plot(mpg01,cylinders, xlabel = "mpg01",ylab = "engine cylinders")
plot(mpg01,displacement,xlabel = "mpg01",ylab = "Engine displacement")
plot(mpg01,horsepower,xlabel = "mpg01",ylab = "Horsepower")
plot(mpg01,weight, xlabel = "mpg01",ylab = "Weight")
plot(mpg01,acceleration, xlabel = "mpg01",ylab = "accelaration")
plot(mpg01,year,xlabel = "mpg01",ylab = "Manufacture year")
mtext("Scatterplots", outer = TRUE, line = -1.5)
```

From the scatterplot above, the predictor`displacement`, `horsepower` and `weight` seem to be good variables to predict `mpg01`.

**To sum up, `cylinder`,`displacement`, `horsepower` and `weight` are the strongest variable to predict `mpg0`.**

c.Split the data into a training set and a test set.

Split the Auto data set into 75% trainning sample and 25% testing sample with no replacement.
```{r}
set.seed(1) 
index = sample.int(n = nrow(Auto), size = floor(.75*nrow(Auto)), replace = F)
train = Auto[index, ]
test  = Auto[-index, ]
```

d.Perform LDA

```{r}
library(MASS)
lda.fit = lda(mpg01~cylinders+weight+displacement+horsepower,data = Auto,subset = index)
lda_pred = predict(lda.fit, test)
mean(lda_pred$class != Auto[-index, "mpg01"])
```
The test error of LDA is 13.27%

e.Perform QDA
```{r}
library(MASS)
qda.fit = qda(mpg01~cylinders+weight+displacement+horsepower,data = Auto,subset = index)
qda_pred = predict(qda.fit, test)
mean(qda_pred$class != Auto[-index, "mpg01"])
```
The test error of QDA is 12.24%

f.Perform logistic regression
```{r}
glm.fit = glm(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, family = binomial, subset = index)
glm.probs = predict(glm.fit, test, type = "response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs > 0.5] = 1
mean(glm.pred != Auto[-index, "mpg01"])
```
The test error for logistic regression is 10.20%

g. Perform KNN with several values of K.

First try k = 1.
```{r}
library(class)
standardized.X=scale(Auto[, -c(8, 9, 10)])
col = c("cylinders", "displacement", "horsepower", "weight")
train.X=standardized.X[index , col]
test.X=standardized.X[-index , col]
train.Y = Auto[index, "mpg01"]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Y, k = 1)
mean(knn.pred != Auto[-index, "mpg01"])
```
The test error for knn when k = 1 is 10.20%

Then we can try k = 3.
```{r}
knn.pred = knn(train.X, test.X, train.Y, k = 3)
mean(knn.pred != Auto[-index, "mpg01"])
```
The test error for knn when k = 3 is 13.27%

Then try k = 5.
```{r}
knn.pred = knn(train.X, test.X, train.Y, k = 5)
mean(knn.pred != Auto[-index, "mpg01"])
```
The test error for knn when k = 5 is 14.29%

Try k = 10
```{r}
knn.pred = knn(train.X, test.X, train.Y, k = 10)
mean(knn.pred != Auto[-index, "mpg01"])
```
The test error for knn when k = 10 is 12.24%

K = 1 works better for KNN.