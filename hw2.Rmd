---
title: "PH.140.644_HW2"
author: "Kunyu An"
output:
  html_document:
    toc_depth: '2'
    df_print: paged
  pdf_document:
    toc_depth: 2
    latex_engine : xelatex
header-includes:
- \usepackage{amsmath,latexsym,amsfonts,amsthm,cleveref}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


```{r load_libraries,include=FALSE}
library(ISLR)
library(MASS) 
library(class)
```

### Chapter 5
#### Q1
\begin{equation}
\begin{split}
Var(\alpha X, (1-\alpha)Y)&=\alpha^{2}\sigma_{X}^{2}+2\alpha(1-\alpha)\sigma_{XY}+(1-\alpha)^{2}\sigma_{Y}^{2}\\
&=(\sigma_{X}^{2}-2\sigma_{XY}+\sigma_{Y}^{2})\alpha^{2}+2(\sigma_{XY}-\sigma_{Y}^{2})\alpha+\sigma_{Y}^{2}
\end{split}
\end{equation}
Let
\begin{equation}
\frac{\partial Var(\alpha X, (1-\alpha)Y)}{\partial \alpha}=2(\sigma_{X}^{2}-2\sigma_{XY}+\sigma_{Y}^{2})\alpha+2(\sigma_{XY}-\sigma_{Y}^{2})=0
\end{equation}
We have
\begin{equation}
\alpha = \frac{\sigma_{Y}^{2} - \sigma_{XY}}{\sigma_{X}^{2} + \sigma_{Y}^{2} - 2 \sigma_{XY}}    
\end{equation}
We also have
\begin{equation}
\frac{\partial^{2} Var(\alpha X, (1-\alpha)Y)}{\partial \alpha^{2}}=2(\sigma_{X}^{2}-2\sigma_{XY}+\sigma_{Y}^{2})\geq 0
\end{equation}
To sum up, when $\alpha = \frac{\sigma_{Y}^{2} - \sigma_{XY}}{\sigma_{X}^{2} + \sigma_{Y}^{2} - 2 \sigma_{XY}}$, $Var(\alpha X, (1-\alpha)Y)$ has minimum.

#### Q3 
We now review k-fold cross-validation.

**a.**Explain how k-fold cross-validation is implemented.

For k-fold cross validation, we first need to divide the sample randomly into k equal-size and non-overlapping groups. Then, every time we use one group as the validation set and the rest as a whole to be the training set. Finally we can compute the testing error by averaging over the statistics of interest in k experiments.

**b.**what are the advantages and disadvantages of k-fold cross-
validation relative to:

*i. The validation set approach?*

1. The k-fold cross validation has a much lower variability than the validation set approach. 

2. All the data is used to both train and test model performance for k-fold CV.

3. The validation set approach is much easier to understand and to preform. A model is only trained once and tested once. In k-fold CV, k models will be trained and tested.

*ii. LOOCV?*

1. k-fold cross validation is less computationally demanding then LOOCV.
2. LOOCV has higher variance than k-fold CV.

### Chapter 6
#### Q1
